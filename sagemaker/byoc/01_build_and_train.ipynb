{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a4e4c9-71ef-4891-ba2d-96e1b384c2b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Building and Training Your Container\n",
    "\n",
    "## System Setup\n",
    "\n",
    "This notebook is designed to run on **Amazon Linux**, and it includes several setup steps necessary for our project. Our objective is to train a Reinforcement Learning model using the LunarLander-v2 environment. To visually analyze the performance of our lander, we need to create videos of its execution. For this purpose, we'll install several dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4040dbc-e7bf-4375-9b7b-0ec3181813b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Operating System: Amazon Linux 2\n"
     ]
    }
   ],
   "source": [
    "!hostnamectl | grep \"Operating System\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7a4d3-74bf-4846-b0d3-b54a8251336f",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this section, we will set up all necessary tools and libraries for our reinforcement learning environment. We begin by installing several crucial Python libraries, followed by `ffmpeg` for video processing.\n",
    "\n",
    "### Installing Python Libraries\n",
    "\n",
    "We need to install a set of Python libraries that are integral to our project's reinforcement learning environment and model training:\n",
    "\n",
    "- **`swig`**: this tool facilitates the creation of Python bindings to C and C++ code, which is crucial for interacting with some components of the `gymnasium` library.\n",
    "- **`gymnasium[box2d]`**: provides the LunarLander-v2 environment, our primary simulation tool for this project.\n",
    "- **`stable-baselines3`**: offers a suite of improved implementations of reinforcement learning algorithms, based on OpenAI Baselines, which help us efficiently train our model.\n",
    "- **`opencv-python`**: used for processing video frames, enabling us to visually assess and document the performance of our training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e7bac-c2b9-4708-a101-2b1de1dce471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install swig==4.2.1\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install stable-baselines3==2.0.0a5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534d608-6fb9-4792-8c01-185e6433fd4e",
   "metadata": {},
   "source": [
    "### Installing ffmpeg\n",
    "\n",
    "After setting up our Python environment, we install `ffmpeg`, which is essential for processing videos. This tool allows us to capture and view the agent's maneuvers as it tries to land. Specifically, we use ffmpeg to utilize the H.264 codec, enabling us to embed high-quality video outputs directly into our Jupyter notebooks.\n",
    "\n",
    "Execute the following commands in your notebook to run the installation script located in the `/utils` directory:\n",
    "\n",
    "```bash\n",
    "chmod +x ./utils/install_ffmpeg.sh\n",
    "sudo ./utils/install_ffmpeg.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29262be-8613-4060-9216-d825373e7942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from utils.video_render import record_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8857d27-b5a0-4075-a12e-5c96ca71b0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c841bb-7a87-4b34-aa98-92c8f3958e16",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "In Lunar Lander v2, our agent's task is to control a lander aiming for a smooth, safe landing at a designated spot. The environment mimics real-life physics, challenging you to manage the landerâ€™s movements meticulously to avoid a crash.\n",
    "\n",
    "Let's safely land a spacecraft on the moon! ðŸš€ðŸŒ‘âœ¨\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The agent receives an array of eight observations from the environment, giving a comprehensive view of the current state:\n",
    "\n",
    "- **Positions**: Horizontal and vertical.\n",
    "- **Velocities**: Horizontal and vertical.\n",
    "- **Angle**: Orientation of the lander.\n",
    "- **Angular Velocity**: Speed of rotation.\n",
    "- **Leg Contact**: Boolean values for each leg indicating contact with the ground.\n",
    "\n",
    "These parameters vary within defined ranges, such as positions ranging from -90 to 90, providing detailed feedback on the lander's status.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action space comprises four discrete actions:\n",
    "\n",
    "1. **Do nothing**.\n",
    "2. **Fire left orientation engine**.\n",
    "3. **Fire main engine**.\n",
    "4. **Fire right orientation engine**.\n",
    "\n",
    "Strategic actions are required to control the landerâ€™s engines effectively, ensuring a gentle touchdown.\n",
    "\n",
    "### Goals\n",
    "\n",
    "Our goal is to train a reinforcement learning **model** that can:\n",
    "\n",
    "- Achieve a safe landing close to the target.\n",
    "- Minimize fuel consumption.\n",
    "- Avoid any crashes.\n",
    "\n",
    "<div style=\"border:2px solid #42A891; padding: 10px; background-color: #E0F2F1; border-radius: 5px;\">\n",
    "    <b>LEARN MORE:</b> If you're interested in learning more about Deep Reinforcement Learning, check out this <a href=\"https://huggingface.co/learn/deep-rl-course/en/unit0/introduction\" target=\"_blank\" style=\"font-weight: bold;\">AMAZING course</a> made by HuggingFace ðŸ¤—\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb369bde-3ef5-4a3c-a8c2-d75845f9e3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENVIRONMENT: Lunar Lander v2\n",
      "â”‚\n",
      "â”œâ”€â”€ OBSERVATION SPACE\n",
      "â”‚   â”œâ”€â”€ Shape: (8,)\n",
      "â”‚   â”œâ”€â”€ Highest Values: [90.0, 90.0, 5.0, 5.0, 3.1415927, 5.0, 1.0, 1.0]\n",
      "â”‚   â””â”€â”€ Lowest Values: [-90.0, -90.0, -5.0, -5.0, -3.1415927, -5.0, -0.0, -0.0]\n",
      "â”‚\n",
      "â””â”€â”€ ACTION SPACE\n",
      "    â””â”€â”€ Discrete(4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( f\"\"\"\n",
    "ENVIRONMENT: Lunar Lander v2\n",
    "â”‚\n",
    "â”œâ”€â”€ OBSERVATION SPACE\n",
    "â”‚   â”œâ”€â”€ Shape: {env.observation_space.shape}\n",
    "â”‚   â”œâ”€â”€ Highest Values: {list(env.observation_space.high)}\n",
    "â”‚   â””â”€â”€ Lowest Values: {list(env.observation_space.low)}\n",
    "â”‚\n",
    "â””â”€â”€ ACTION SPACE\n",
    "    â””â”€â”€ {env.action_space}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f22aa-e42e-4cd7-88b3-6126e1a5c0f5",
   "metadata": {},
   "source": [
    "## Exploring Random Actions\n",
    "\n",
    "Before we dive into sophisticated training algorithms, let's start with a simple experiment. We'll see how the lunar lander performs under completely random controls. This will give us a baseline understanding of the challenges involved in landing safely.\n",
    "\n",
    "### Why Random Actions?\n",
    "\n",
    "Using random actions allows us to observe the inherent difficulty of the task and the behavior of the lander in various uncontrolled scenarios. It's like throwing a novice into the pilot's seatâ€”exciting, unpredictable, and a bit chaotic! By seeing how the lander behaves when actions are chosen without any strategy, we can better appreciate the complexity of the task and the importance of a well-trained model.\n",
    "\n",
    "### Implementing Random Actions\n",
    "\n",
    "We'll implement a function named `random_action`, which will select an action randomly from the available action space at each step of the simulation. This function takes the current environment and observation as inputs and returns a random action. This method simulates an unsophisticated approach to controlling the lander, providing us with a clear picture of what not to do.\n",
    "\n",
    "Letâ€™s see how our lander fares with randomness at the helm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb369e80-e873-4e2f-b527-63a6b0770246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_action(env: gym.Env, obs: np.ndarray) -> int:\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ec8739-fa09-44d8-9e9f-ad4c588e9c70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video conversion successful: ./videos/random.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./videos/random.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "video_path = './videos/random.mp4'\n",
    "\n",
    "record_simulation(random_action, video_path)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fbc144-f43c-4627-9b94-dcd954be9e0e",
   "metadata": {},
   "source": [
    "## SageMaker Custom Containers: Training and Inference\n",
    "\n",
    "As we embark on our quest to train a model for the lunar landing, we'll utilize the robust AWS SageMaker service. SageMaker streamlines the process of constructing, training, and deploying machine learning models at a large scale. But to tailor this process to our needs, we'll introduce our own containers.\n",
    "\n",
    "### Custom Training Container\n",
    "\n",
    "Within SageMaker, we have the capability to deploy **custom Docker containers for training jobs**. This allows us the freedom to specify our environment to the finest detail, incorporating all the necessary dependencies our model requires, from specific library versions to custom code snippets.\n",
    "\n",
    "In the diagram below, you will see how a custom training image stored in Amazon Elastic Container Registry (ECR) is utilized by SageMaker. When a training job is invoked, SageMaker retrieves this image to run our training job, resulting in the generation of a model artifact.\n",
    "\n",
    "![SageMaker Training Workflow](./img/byoc.png)\n",
    "\n",
    "### Custom Inference Container\n",
    "\n",
    "After training our model, the subsequent step is deployment. For inference purposes, we employ another Docker container designed to serve our model. This container is fine-tuned for efficiently processing incoming prediction requests rather than training.\n",
    "\n",
    "Deploying our model to a SageMaker Endpoint prompts SageMaker to fetch the inference image and establish a scalable, consistent service endpoint for real-time predictions.\n",
    "\n",
    "**NOTE:** it is also possible to have a single image that is used for both training and model deployment. SageMaker runs a different command depending on whether you're calling a training job or a deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1253ac-0513-4ca1-80c3-3a4d7ef3b776",
   "metadata": {},
   "source": [
    "## Building the Training Image\n",
    "\n",
    "Within the `training_container/` directory, you'll find all the ingredients needed to cook up your custom training image. Let's sift through these components to understand their purpose and how they come together.\n",
    "\n",
    "```plaintext\n",
    "training_container/\n",
    "â”‚\n",
    "â”œâ”€â”€ train\n",
    "â”œâ”€â”€ Dockerfile\n",
    "â””â”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "### train\n",
    "\n",
    "At the heart of the training container is the train script. When SageMaker initiates a training job, it runs your container by executing `docker run`**`image`**`train`. You have the option to change the default behavior by modifying the container's ENTRYPOINT, but in our case, simplicity is key.\n",
    "\n",
    "Our script employs the [PPO (Proximal Policy Optimization)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm from the Stable Baselines3 library to train our model. Below is a snapshot of the script's structure:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "#import dependencies\n",
    "\n",
    "class ParamsParser(BaseModel):\n",
    "    \"\"\"\n",
    "    Parses and validates parameters for the PPO model configuration from a JSON file.\n",
    "    Provides default values and type enforcement.\n",
    "    \"\"\"\n",
    "    seed: int = None\n",
    "    n_steps: int = 1024\n",
    "    learning_rate: float = 0.0003\n",
    "    ...\n",
    "```\n",
    "\n",
    "The script uses Pydantic to validate the hyperparameters from the `Estimator` object in SageMaker. These are provided in a JSON file located at `/opt/ml/input/config/hyperparameters.json`.\n",
    "\n",
    "```python\n",
    "def train() -> None:\n",
    "    \"\"\"\n",
    "    Initializes and executes the training process for a PPO model using parameters\n",
    "    from a JSON configuration file. Designed for use in a BYOC SageMaker training job,\n",
    "    handling environment setup, model training, and model saving.\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    model_path = '/opt/ml/model'  # 'SM_MODEL_DIR'\n",
    "    params_path = '/opt/ml/input/config/hyperparameters.json'\n",
    "\n",
    "    # Parameter parsing with Pydantic\n",
    "    ...\n",
    "\n",
    "    # Training Environment\n",
    "    env = make_vec_env('LunarLander-v2', n_envs=16)\n",
    "    \n",
    "    # Set device\n",
    "    if torch.cuda.is_available():\n",
    "        ...\n",
    "    \n",
    "    # Learning Algorithm\n",
    "    model = PPO(\n",
    "        policy='MlpPolicy',\n",
    "        env=env,\n",
    "        n_steps=params.n_steps,\n",
    "        learning_rate=params.learning_rate,\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    # Training and saving artifact in '/opt/ml/model'\n",
    "    ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n",
    "```\n",
    "\n",
    "After training, SageMaker will compress the contents of `/opt/ml/model` into a `tar.gz` file and save it to the output path (i.e., an S3 URI) defined in the `Estimator`.\n",
    "\n",
    "<div style=\"border:2px solid #42A891; padding: 10px; background-color: #E0F2F1; border-radius: 5px;\">\n",
    "    <b>LEARN MORE:</b> For detailed insights on how Amazon SageMaker processes training output, explore the corresponding <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-output.html\" target=\"_blank\" style=\"font-weight: bold;\">SageMaker docs</a>.\n",
    "</div>\n",
    "\n",
    "\n",
    "### Dockerfile and requirements.txt\n",
    "\n",
    "The `requirements.txt` file lists the necessary Python libraries for our environment. A special note on `gymnasium`: it requires `swig` to be installed beforehand, which we handle separately from the `requirements.txt`.\n",
    "\n",
    "The Dockerfile is simple: it starts with a base Python image, installs the dependencies, and ensures the train script is ready to run. Without an `ENTRYPOINT`, we stick with the default convention where the script's name must be `train`.\n",
    "\n",
    "```Dockerfile\n",
    "FROM --platform=linux/amd64 python:3.10\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies\n",
    "RUN pip install -U pip\n",
    "RUN pip install swig && pip install gymnasium[box2d]\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "ENV SM_MODEL_DIR /opt/ml/model\n",
    "\n",
    "COPY . /opt/program\n",
    "\n",
    "WORKDIR /opt/program\n",
    "\n",
    "# Sagemaker runs your container by runnning: docker run <image> train\n",
    "RUN chmod +x /opt/program/train\n",
    "\n",
    "ENV PATH=\"/opt/program:${PATH}\"\n",
    "```\n",
    "\n",
    "<div style=\"border:2px solid #42A891; padding: 10px; background-color: #E0F2F1; border-radius: 5px;\">\n",
    "    <b>LEARN MORE:</b> Discover how Amazon SageMaker runs your training image by checking out the corresponding <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-dockerfile.html\" target=\"_blank\" style=\"font-weight: bold;\">SageMaker docs</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e2ff90-1bde-4d71-9e43-468bd047c75d",
   "metadata": {},
   "source": [
    "## Build the training image\n",
    "\n",
    "Now build your container and push it to your own ECR repository. Create your own private ECR repository and push the image. If you're not experienced with this, you can find the push commands here after you create the repository:\n",
    "\n",
    "![](./img/ecr_push.png)\n",
    "\n",
    "<div style=\"border:2px solid #42A891; padding: 10px; background-color: #E0F2F1; border-radius: 5px;\">\n",
    "    <b>LEARN MORE:</b> Read about pushing images to ECR <a href=\"https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\" target=\"_blank\" style=\"font-weight: bold;\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f9c9a-0fb5-42af-a06a-d5c01347e15c",
   "metadata": {},
   "source": [
    "## Enter: SageMaker\n",
    "\n",
    "<div style=\"border:2px solid #FFA500; padding: 10px; background-color: #FFF9C4; border-radius: 5px;\">\n",
    "    <b>IMPORTANT:</b> Ensure that you update the following code chunk with your specific details: Replace <code>role</code> with your execution role that has the required permissions, such as <a href=\"https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSageMakerFullAccess.html\" target=\"_blank\" style=\"font-weight: bold;\">SagemakerFullAccess</a>. Update <code>image_uri</code> with the URI from your own ECR repository. The URI format should be <code>arn:aws:ecr:<b>aws-region</b>:<b>account-id</b>:repository/<b>repository-name</b>:<b>image-tag</b></code>. Lastly, substitute <code>output_path</code> with the address of your personal S3 bucket.\n",
    "</div>\n",
    "\n",
    "### Estimator\n",
    "\n",
    "It's time to define our SageMaker Estimator, the driving force of our training job. The Estimator requires a few key configurations: the Docker image URI (`image_uri`), which points to our custom image in ECR; the instance specifications like *count* and *type*, determining our compute resources; and the `output_path`, directing where the trained model artifacts will be stored in S3.\n",
    "\n",
    "Within the Estimator, we also set our training `hyperparameters`, aligning with those expected by our training script's `ParamsParser`. Additionally, we define `metric_definitions` to track key performance metrics from the training logs.\n",
    "\n",
    "In the upcoming section, we'll explore how these metric configurations come into play as SageMaker orchestrates our model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc698223-4036-4f5c-8426-b75104e8ac7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from utils.helpers import get_secret\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "metric_definitions = [\n",
    "    {'Name': 'ep_len_mean', 'Regex': 'ep_len_mean\\s*\\|\\s*([\\d\\.]+)'},\n",
    "    {'Name': 'ep_rew_mean', 'Regex': 'ep_rew_mean\\s*\\|\\s*(-?[\\d\\.]+)'}\n",
    "]\n",
    "\n",
    "ecr_repo = get_secret('ecr_repository')\n",
    "\n",
    "estimator = Estimator(\n",
    "    role=get_secret('training_execution_role'),\n",
    "    image_uri=f'{ecr_repo}:lunar_lander_training',\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    output_path=get_secret('s3_bucket_uri'),\n",
    "    session=session,\n",
    "    hyperparameters={\n",
    "        'seed': 1020,\n",
    "        'total_timesteps': 1500000\n",
    "    },\n",
    "    metric_definitions=metric_definitions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d932c93-8a18-4355-8532-dda1728fe44f",
   "metadata": {},
   "source": [
    "### Metrics Definition\n",
    "\n",
    "During training, our PPO model outputs various performance metrics to the logs, which SageMaker automatically monitors. These logs are printed to stdout, and from there, SageMaker relays the data to [Amazon CloudWatch](https://aws.amazon.com/es/cloudwatch/) in real-time. To capture specific metrics, we utilize regular expressions that match the patterns in these logs.\n",
    "\n",
    "For instance, the PPO model **periodically** prints a summary table of metrics:\n",
    "\n",
    "    ----------------------------------------\n",
    "    | rollout/                |            |\n",
    "    |    ep_len_mean          | 661        | <--- (our target metric)\n",
    "    |    ep_rew_mean          | 57.1       | <--- (our target metric)\n",
    "    | time/                   |            |\n",
    "    |    fps                  | 1449       |\n",
    "    |    iterations           | 19         |\n",
    "    |    time_elapsed         | 214        |\n",
    "    |    total_timesteps      | 311296     |\n",
    "    | train/                  |            |\n",
    "    |    approx_kl            | 0.00773607 |\n",
    "    |    clip_fraction        | 0.0484     |\n",
    "    |    clip_range           | 0.2        |\n",
    "    |    entropy_loss         | -1.17      |\n",
    "    |    explained_variance   | 0.748      |\n",
    "    |    learning_rate        | 0.0003     |\n",
    "    |    loss                 | 43.7       |\n",
    "    |    n_updates            | 72         |\n",
    "    |    policy_gradient_loss | -0.0025    |\n",
    "    |    value_loss           | 183        |\n",
    "    ----------------------------------------\n",
    "\n",
    "To track the average episode length (`ep_len_mean`), we use the regex `'ep_len_mean\\s*\\|\\s*([\\d\\.]+)'`, which isolates and captures the numerical value following the metric's name.\n",
    "\n",
    "By setting up metric definitions in our SageMaker Estimator with these regular expressions, we enable the capture and tracking of specific metrics like `ep_len_mean` and `ep_rew_mean` during the training process. This data is then available in CloudWatch for real-time analysis and post-training evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d726036-e102-412d-b415-a1de78ebe546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_job_name = 'lunar-lander-PPO-training'\n",
    "\n",
    "estimator.fit(job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd84b3-961c-4f2b-b992-688ffee24193",
   "metadata": {},
   "source": [
    "### Monitoring Training\n",
    "\n",
    "Once you've initiated the training job in SageMaker, you can track its progress and performance through the AWS Console. Navigate to SageMaker > Training > Training Jobs to find your active job.\n",
    "\n",
    "![Training Job Overview](./img/training_job.png)\n",
    "\n",
    "On the job's dashboard, you'll see both instance and algorithm metrics, which provide insights into the resource usage and the learning progress respectively.\n",
    "\n",
    "![Training Job Metrics](./img/training_job_metrics.png)\n",
    "\n",
    "For an in-depth look at your model's performance metrics, such as average episode length and reward, head over to Amazon CloudWatch. Here, you can interact with the metrics over time, drill down into specifics, and even export the data for further analysis.\n",
    "\n",
    "![CloudWatch Metrics Detail](./img/cloudwatch_metrics.png)\n",
    "\n",
    "We observe in the algorithm metrics an increase in the average episode length (`ep_len_mean`). This trend suggests that the model, while learning, is being more cautious and taking its time to land the spacecraft, possibly exploring various strategies to find a safe path to the lunar surface. This period of exploration is important as the agent needs to understand the consequences of its actions within the environment.\n",
    "\n",
    "Subsequently, there's a noteworthy shift in the pattern. The average reward starts to rise consistently, implying that the model has begun to optimize its movements. It discovers more efficient ways to land the spacecraft, achieving higher rewards (`ep_rew_mean`) in shorter amounts of time. The decline in episode length paired with the increase in rewards is a strong indicator of the model learning and improving its landing strategy.\n",
    "\n",
    "## Fetching the artifact\n",
    "\n",
    "<div style=\"border:2px solid #FFA500; padding: 10px; background-color: #FFF9C4; border-radius: 5px;\">\n",
    "    <b>IMPORTANT:</b> Replace the function call <code>get_secret('s3_bucket_name')</code> with the actual name of your own S3 bucket.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Great job on completing the training! SageMaker has now bundled the training results from `/opt/ml/model/` into a single `tar.gz` file, which can be found in your S3 bucket. This file is your model's learned wisdom, all set for deployment or further evaluation.\n",
    "\n",
    "To get your hands on the model artifact, we'll use the `boto3` library. It's the standard toolkit provided by AWS for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63326698-c756-444d-8532-6e8d370151bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(get_secret('s3_bucket_name'),\n",
    "                 f'{training_job_name}/output/model.tar.gz', './artifacts/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560fd54-9742-44fc-afc4-893e170fdc7c",
   "metadata": {},
   "source": [
    "Let's get our model into action! First, we'll load the trained model by extracting it from the model.tar.gz file. Next, we grab a random observation from the environment to mimic a new scenario for our lunar lander. With the model loaded and an observation in hand, we'll predict the best action for the lander. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77f33b9-3c7c-4062-b68a-236350bbfc20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf ./artifacts/model.tar.gz -C ./artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ebda0a-0a71-4fef-b548-2437ffeee51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO.load('./artifacts/model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dee43a5-d733-47ce-86bf-408654fdd274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-87.45102    -33.222546    -4.865471     3.340176    -1.3296702\n",
      "  -0.22886698   0.9492815    0.87525284]\n"
     ]
    }
   ],
   "source": [
    "observation_sample = env.observation_space.sample()\n",
    "print(observation_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0bd4a6-e226-43dc-9423-a5f14e050e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "action, _ = model.predict(observation_sample)\n",
    "\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac1172-4360-40d1-a4dc-6057f1f085b2",
   "metadata": {},
   "source": [
    "The moment has arrived to put our trained model to the test. We'll load it into the environment and observe how it handles the task it was trained for ðŸŒ™ðŸš€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e17ad9-2aa2-408c-8b7e-08f87df7f86c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video conversion successful: ./videos/model.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./videos/model.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.video_render import make_model_action\n",
    "\n",
    "video_path = './videos/model.mp4'\n",
    "\n",
    "model_action = make_model_action(model)\n",
    "record_simulation(model_action, video_path)\n",
    "Video(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dd033-8395-43b2-a4a4-daf6562ece36",
   "metadata": {},
   "source": [
    "Congratulations on a successful mission! You've effectively navigated through building, training, and evaluating a reinforcement learning model that's mastered the delicate art of lunar landing. This journey has given us insights into the powerful capabilities of AWS SageMaker and the intricacies of model training. Ready for the next adventure? In Part 2, we'll deploy our well-trained model as a SageMaker endpoint, bringing our lunar expertise into the realm of real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310753b-88ff-45e9-a0c9-7a61c8318e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
